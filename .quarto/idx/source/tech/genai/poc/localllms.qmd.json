{"title":"Local LLMs","markdown":{"yaml":{"title":"Local LLMs"},"headingText":"The Buzzwords","containsRefs":false,"markdown":"\n\n\n-   **GenAI**\n    -   Became very common topic\n-   **GPT-4, Gemini etc**\n    -   GPT-3.5 shade away, new service/product coming every day\n-   **OpenAI, Google, Meta**\n    -   No more big companies, even Academias also started releasing (eg.Falcon, LLaVa)\n-   **LLM, LMM, soon LxM**\n    -   No more Language, Vision, Audio and Multi models are coming\n-   **RAG**\n    -   Common Pattern for injecting new knowledge\n-   **Frameworks - LangChain, LlamaIndex, Streamlit, Gradio etc**\n    -   Became defacto frameworks for GenAI app development\n\n------------------------------------------------------------------------\n\n### Cloud Based Services\n\n-   Popular LLMs are available as Commercial & Cloud based services (OpenAI, Google, AWS, IBM)\n\n-   Large Model size & Needs high computation power\n\n-   Offers Trial Versions but with Rate Limits\n\n-   Data Privacy & Security concerns as data go outside\n\n-   There are other Options available\n\n    -   Open Source (freely accessible)\n    -   Commercially Usable\n    -   Self-Hosted\n    -   Deployable on Commodity Hardware (for prototyping & evaluation)\n\n------------------------------------------------------------------------\n\n### Local/On-Prem LLMs\n\n-   Open Source Models - Llama2, Falcon, T5 etc\n\n-   Model Quantization\n\n    -   reduced model sizes by reducing weights from high precision to lower\n    -   run on commodity hardware\n\n-   Llama.cpp made it possible\n\n-   New model Quantization formats introduced - GGUF and now GGML\n\n-   We can run models on CPUs\n\n    -   No more GPUs needed, they are just a performance boosters\n\n-   Frameworks like Llama.cpp (Python binding), Ollama, GPT4All made it possible\n\n-   But what it takes to run them locally...\n\n    -   will share the experience on Ollama, GPT4All and Llama.cpp\n\n------------------------------------------------------------------------\n\n### Llama.cpp\n\n-   C/C++ interface for running Llama model\n-   binding is available for Python echo system\n-   Runs on most of the platforms (Windows, Linux, MacOS and Docker)\n-   Supports CPU & GPU but need to compile sources for GPU\n-   Plenty of configuration options beyond obvious temp, n_ctx, top_p, top_k etc\n-   Supports 25+ models, 4 multi models, has all major programming language bindings.\n-   [**Observations**]{style=\"color: maroon;\"}\n    -   API based, Easy to use\n    -   No mechanism of auto model download\\\n    -   A bit of slow compared to others frameworks\n\n``` python\nfrom llama_cpp import Llama\n\nllm = Llama(model_path=\"models/llama-2-7b.Q4_K_M.gguf\")\noutput = llm(\"Q: How can you greet someone in Sanskrit? A:\")\nprint(output)\n\n# Output: {'id': 'cmpl-e9db8788-d78f-4832-beb4-74d4f257be3b', 'object': 'text_completion', 'created': 1705823100, 'model': 'models/llama-2-7b.Q4_K_M.gguf', 'choices': [{'text': ' Namaste. surely one of the most well-known greetings from India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 16, 'total_tokens': 32}}\n```\n\n------------------------------------------------------------------------\n\n### Ollama\n\n-   a tool to run LLMs locally\n-   Runs as a Service and exposes API to interact with models\n-   Provides CLI for managing models\n-   Runs only on Linux and MacOS. Windows support is coming soon\n-   Automatically pulls model through CLI\n-   We can fine-tune and create new custom models just like docker images\n-   Need to interact with Models using REST Api served on `localhost:11434`\n-   Supports majority of models and on both CPU and GPUs\n-   Python binding available `ollama-python` along with LangChain\n\n``` bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"How to greet someone in Sanskrit?\",\n  \"stream\":false\n}'\n#{\"model\":\"llama2\",\"created_at\":\"2024-01-21T09:19:18.746918026Z\",\"response\":\"\\nIn Sanskrit, there are several ways to greet someone, depending on the time of day and the level of formality you want to convey. Here are some common Sanskrit greetings:\\n\\n1. \\\"Namaskāra\\\" (नमस्कार): This is a common greeting in Sanskrit, which can be translated to \\\"I bow to you.\\\" It is often used as a general greeting, especially during the daytime.\\n2. \\\"Prāpta\\\" (प्राप्ति): This greeting is used when you want to show respect or gratitude towards someone. It can be translated to \\\"Blessings upon you.\\\"\\n3. \\\"Dhanyavaad\\\" (धन्यवाद): This word means \\\"Thank you\\\" in Sanskrit, and it is often used as a greeting, especially when you want to express your gratitude towards someone.\\n4. \\\"Jaya\\\" (जय): This is a more informal greeting in Sanskrit, which can be translated to \\\"Victory\\\" or \\\"Well done.\\\" It is often used among friends or peers.\\n5. \\\"Shraddhā\\\" (श्रद्धा): This word means \\\"Devotion\\\" or \\\"Respect\\\" in Sanskrit, and it is often used as a greeting when you want to show your devotion or respect towards someone.\\n6. \\\"Maitrī\\\" (मैत्री): This is a friendly greeting in Sanskrit, which can be translated to \\\"Friendship\\\" or \\\"Blessings.\\\" It is often used among friends or peers.\\n7. \\\"Vidhāt\\\" (विधात): This word means \\\"Wishing you\\\" in Sanskrit, and it is often used as a greeting when you want to express your good wishes towards someone.\\n8. \\\"Sukhino\\\" (सुखिनो): This word means \\\"May you be happy\\\" in Sanskrit, and it is often used as a greeting when you want to wish someone happiness or well-being.\\n9. \\\"Bhāvya\\\" (भाव्य): This word means \\\"Wishing you well\\\" in Sanskrit, and it is often used as a greeting when you want to express your good wishes towards someone.\\n10. \\\"Satya\\\" (सत्य): This word means \\\"Truth\\\" or \\\"Reality\\\" in Sanskrit, and it is often used as a greeting when you want to convey the idea of truthfulness or sincerity towards someone.\\n\\nRemember that Sanskrit is a rich and complex language, and there are many other words and phrases that can be used as greetings depending on the context and level of formality you want to convey.\",\"done\":true,\"context\":[518,25580,29962,3532,14816,29903,29958,5299,829,14816,29903,6778,13,13,5328,304,1395,300,4856,297,317,12190,768,29973,518,29914,25580,29962,13,13,797,317,12190,768,29892,727,526,3196,5837,304,1395,300,4856,29892,8679,373,278,931,310,2462,322,278,3233,310,883,2877,366,864,304,27769,29889,2266,526,777,3619,317,12190,768,1395,300,886,29901,13,13,29896,29889,376,29940,314,1278,30107,336,29908,313,30424,30485,30489,30296,30444,30269,30316,1125,910,338,263,3619,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,29902,12580,304,366,1213,739,338,4049,1304,408,263,2498,1395,15133,29892,7148,2645,278,2462,2230,29889,13,29906,29889,376,4040,30107,28363,29908,313,30621,30296,30316,30269,30621,30296,30475,30436,1125,910,1395,15133,338,1304,746,366,864,304,1510,3390,470,20715,4279,7113,4856,29889,739,508,367,20512,304,376,29933,2222,886,2501,366,1213,13,29941,29889,376,29928,29882,1384,879,328,29908,313,31437,30424,30296,30640,30610,30269,30694,1125,910,1734,2794,376,25271,366,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,29892,7148,746,366,864,304,4653,596,20715,4279,7113,4856,29889,13,29946,29889,376,29967,9010,29908,313,30871,30640,1125,910,338,263,901,1871,284,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,29963,919,706,29908,470,376,11284,2309,1213,739,338,4049,1304,4249,7875,470,1236,414,29889,13,29945,29889,376,29903,1092,1202,29882,30107,29908,313,31009,30296,30316,30694,30296,31437,30269,1125,910,1734,2794,376,16618,8194,29908,470,376,1666,1103,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,1510,596,2906,8194,470,3390,7113,4856,29889,13,29953,29889,376,29924,1249,29878,30150,29908,313,30485,31678,30475,30296,30316,30580,1125,910,338,263,19780,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,27034,355,3527,29908,470,376,29933,2222,886,1213,739,338,4049,1304,4249,7875,470,1236,414,29889,13,29955,29889,376,29963,333,29882,30107,29873,29908,313,30610,30436,31437,30269,30475,1125,910,1734,2794,376,29956,14424,366,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,4653,596,1781,28688,7113,4856,29889,13,29947,29889,376,29903,2679,29882,1789,29908,313,30489,30702,31667,30436,30424,30799,1125,910,1734,2794,376,12703,366,367,9796,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,6398,4856,22722,470,1532,29899,915,292,29889,13,29929,29889,376,29933,29882,30107,29894,3761,29908,313,31380,30269,30610,30296,30640,1125,910,1734,2794,376,29956,14424,366,1532,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,4653,596,1781,28688,7113,4856,29889,13,29896,29900,29889,376,29903,271,3761,29908,313,30489,30475,30296,30640,1125,910,1734,2794,376,2308,2806,29908,470,376,1123,2877,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,27769,278,2969,310,8760,1319,2264,470,4457,2265,537,7113,4856,29889,13,13,7301,1096,393,317,12190,768,338,263,8261,322,4280,4086,29892,322,727,526,1784,916,3838,322,12216,2129,393,508,367,1304,408,1395,300,886,8679,373,278,3030,322,3233,310,883,2877,366,864,304,27769,29889],\"total_duration\":163332861673,\"load_duration\":1284681,\"prompt_eval_count\":30,\"prompt_eval_duration\":5579615000,\"eval_count\":621,\"eval_duration\":157747330000}\n```\n\n------------------------------------------------------------------------\n\n### GPT4All\n\n-   from Nomic AI\n-   Provides an echo system from training of a LLM to framework to interact\n-   supports major models including their own trained models\n-   Supports Windows, Linux and Mac Platforms\n-   Provides Python framework for interacting with model\n-   Automatically download models from internet\n-   Supports both CPU and GPUs\n-   [**Observations**]{style=\"color: maroon;\"}\n    -   Better performance than Llama.cpp and Ollama\n    -   Needs more GPU memory if we want to load model into GPU\n    -   Easy to use API\n\n``` python\nfrom gpt4all import GPT4All\n\nllm = GPT4All(model_name=\"mistral-7b-openorca.Q4_0.gguf\", allow_download=False)\noutput = llm.generate(\"Q: How can you greet someone in Sanskrit? A:\")\nprint(output)\n\n#To greet someone in Sanskrit, you can say \"Namaste\" (नमस्ते).\n```\n\n------------------------------------------------------------------------\n\n### P&C of local LLMs\n\n-   **Pros**\n    -   No Subscriptions & Rate Limitations\n    -   Privacy & Security in control. Data will not move anywhere\n    -   Quick Prototype and Evaluation\n    -   More control on turning\n-   **Cons**\n    -   Needs to know internals for turning\n    -   On our own for Deployment & Scalability aspects\n\n------------------------------------------------------------------------\n\n### Environment used for Evaluation\n\n-   **Hardware**\n    -   GPU - Nvidia GeoForce RTX 4 GB\n    -   Intel i5 Processor, 6 core processor\n    -   32 GB RAM\n-   **Software**\n    -   Ubuntu OS\n    -   CUDA Drivers\n    -   Python 3.10\n\n------------------------------------------------------------------------\n\n### Models\n\n-   Llama 2 - Chat & Instructions\n    -   7B & 13B Parameters\n-   CodeLlama 13B - Code generation\n-   Mistral 7B - Chat & Instructions\n-   all-MiniLM-L6-v2-f16 - for Embedding\n-   BakLLaVA - Multi Model for image inferences\n\n------------------------------------------------------------------------\n\n### PoC-1 KnowledgeBase QA Bot (RAG)\n\n::: columns\n::: {.column width=\"50%\"}\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Creating a QA Bot on local documents using Retrieval Augmented Generation using Local LLM.                             |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, Ollama, Streamlit                                                                                           |\n| **Models**       | Llama2 for chat, all-MiniLM-L6-v2-f16 for embedding                                                                    |\n| **Vectorstore**  | FAISS, Chroma                                                                                                          |\n| **Observations** | \\- Chroma stores data in SQLite where FAISS persists data in pickle files                                              |\n|                  | \\- Chroma has decent persist method to persist data in between the ingestion while FAISS need to only once(at the end) |\n|                  | \\- FAISS results are better when copared to Chroma                                                                     |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n:::\n\n::: {.column width=\"50%\"}\n![](./images/rag.png)\n:::\n:::\n\n------------------------------------------------------------------------\n\n### PoC-2 Code documentation Generation\n\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Generating JavaDoc for a Legacy undocumented Java Code                                                                                 |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, Gpt4All                                                                                                                     |\n| **Models**       | CodeLlama for code interpretation, Forge Roaster for Java Source Code parsing                                                          |\n| **Observations** | \\- LLMs tries to modify the source code even though mentioned not to in the prompt                                                     |\n|                  | \\- Balancing the JavaDoc documentation is tricky as part of prompt                                                                     |\n|                  | \\- A Context about the project purpose, components, architecture able to help to add little more meaning to docs.                      |\n|                  | \\- Few Shot Prompting technique helps to give specific instructs like not to generate JavaDoc for Entity accessor and mutator methods. |\n|                  | \\- Can't just trust LLM to modify the original code.                                                                                    |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n\n![](./images/codegen.png){width=\"100%\"}\n\n---\n\n### PoC-3 Data Extraction from PDFs\n\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Extraction of Data from Documents                                                |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, OpenAI, Unstructured                                                  |\n| **Models**       | gpt-3.5                                                                          |\n| **Observations** | \\- Extract data from Amazon Invoice PDF files                                    |\n|                  | \\- OpenAI Functions works well in this scenario                                  |\n|                  | \\- Subsequent process automation can be enabled using Langchain tools and agents |\n|                  | \\- Defining JSON schema will become tricky if Document data is complex           |\n|                  |                                                                                  |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n\n::: columns\n::: {.column width=\"50%\"}\n![](./images/invoice.png){width=\"50%\"}\n:::\n\n::: {.column width=\"50%\"}\n``` json\n{\n   \"Billing Address\":\"Telkapalli Venkata Seshagiri H.No.3-10-21/A, Gokhale Nagar,, Ramanthapur Hyderabad, ANDHRA PRADESH, 500013 IN\",\n   \"PAN No\":\"AAPCA6346P\",\n   \"GST Registration No\":\"36AAPCA6346P1ZW\",\n   \"Order Number\":\"407-9397603-5919514\",\n   \"Invoice Number\":\"IN-HYD8-41962\",\n   \"Invoice Date\":\"08.04.2020\",\n   \"orderItems\":[\n      {\n         \"Sl. No\":\"1\",\n         \"Description\":\"Dark Fantasy Choco Fills, 300g | B01L7A0CU4 ( B01L7A0CU4 )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹83.90\",\n         \"Qty\":\"2\",\n         \"Net Amount\":\"₹167.80\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹15.10\",\n         \"Total Amount\":\"₹198.00\"\n      },\n      {\n         \"Sl. No\":\"2\",\n         \"Description\":\"Madhur Pure Sugar, 5kg Bag | B01GCF0XEY ( B01GCF0XEY )\",\n         \"HSN\":\"1701\",\n         \"Unit Price\":\"₹228.58\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹228.58\",\n         \"Tax Rate\":\"2.5%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹5.71\",\n         \"Total Amount\":\"₹240.00\"\n      },\n      {\n         \"Sl. No\":\"3\",\n         \"Description\":\"Sunfeast Dark Fantasy Choco Fills plus Coffee Fills Combo 75g(Buy 3 Get 1 Free) | B07KGG57RL ( B07KGG57RL )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹76.28\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹76.28\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹6.86\",\n         \"Total Amount\":\"₹90.00\"\n      },\n      {\n         \"Sl. No\":\"4\",\n         \"Description\":\"McVities Fruit Cookie, 120g | B01NCB1OCV ( B01NCB1OCV )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹33.90\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹33.90\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹3.05\",\n         \"Total Amount\":\"₹40.00\"\n      },\n      {\n         \"Sl. No\":\"5\",\n         \"Description\":\"Kellogg's Corn Flakes Real Almond and Honey, 1kg |B01H5LBWG2 ( B01H5LBWG2 )\",\n         \"HSN\":\"1904\",\n         \"Unit Price\":\"₹396.62\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹396.62\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹35.69\",\n         \"Total Amount\":\"₹468.00\"\n      }\n   ],\n   \"Amount in Words\":\"One Thousand And Thirty-six only\"\n}\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Limitations\n\n-   Need more Hardware for faster response and big models\n-   Though Function calling supported in Llama based models, its not as powerful as OpenAI and others\n-   Prompt Format varies between Model, need to get hold of it\n-   Need to get hold of model parameters for fine-tuning\n\n------------------------------------------------------------------------\n\n### LangChain\n\n-   Defacto for LLM orchestration\n-   Very minimal code changes for switching between LLM models\n-   Frequent updates/releases, started with 0.0.34, got new one 0.0.35 and sooner got 0.1.0\n-   Some Code examples refers old modules and classes\n-   Some Examples has issues and will not go quite well if we do on different data\n-   Most of examples are with OpenAI\n-   Most of the documentation is notebooks\n-   One need to have good Python understanding to get hold of source code and code documentation\n-   Debugging with LangSmith is not easy at beginning and waiting list for getting Access Key\n\n------------------------------------------------------------------------\n\n### Skills needed\n\n-   AI/ML background is an optional (very minimal)\n\n-   Good Python programming background is a must\n\n-   If want more adventurous life, sure! LangChain NodeJS framework is a best bet\n\n-   Prompt Engineering theory is entirely different from practical scenarios\n\n-   Hence Intro. to Prompt Engineering or GenAI courses may help to understand terms but not in programming\n\n- Formulating Prompts and Prompt Engineering techniques needs patience. No sure shot syntax\n\n--- \n\n### Conclusion\n\n-   GenAI programming is fun but not quite easy.\n-   Talking about it is different from doing it.\n-   Showing a real value with it takes a while. (well, ChatBot is not really the ones)\n-   But we need to feel it by experience, aka by doing it.\n\n🙏🙏🙏","srcMarkdownNoYaml":"\n\n### The Buzzwords\n\n-   **GenAI**\n    -   Became very common topic\n-   **GPT-4, Gemini etc**\n    -   GPT-3.5 shade away, new service/product coming every day\n-   **OpenAI, Google, Meta**\n    -   No more big companies, even Academias also started releasing (eg.Falcon, LLaVa)\n-   **LLM, LMM, soon LxM**\n    -   No more Language, Vision, Audio and Multi models are coming\n-   **RAG**\n    -   Common Pattern for injecting new knowledge\n-   **Frameworks - LangChain, LlamaIndex, Streamlit, Gradio etc**\n    -   Became defacto frameworks for GenAI app development\n\n------------------------------------------------------------------------\n\n### Cloud Based Services\n\n-   Popular LLMs are available as Commercial & Cloud based services (OpenAI, Google, AWS, IBM)\n\n-   Large Model size & Needs high computation power\n\n-   Offers Trial Versions but with Rate Limits\n\n-   Data Privacy & Security concerns as data go outside\n\n-   There are other Options available\n\n    -   Open Source (freely accessible)\n    -   Commercially Usable\n    -   Self-Hosted\n    -   Deployable on Commodity Hardware (for prototyping & evaluation)\n\n------------------------------------------------------------------------\n\n### Local/On-Prem LLMs\n\n-   Open Source Models - Llama2, Falcon, T5 etc\n\n-   Model Quantization\n\n    -   reduced model sizes by reducing weights from high precision to lower\n    -   run on commodity hardware\n\n-   Llama.cpp made it possible\n\n-   New model Quantization formats introduced - GGUF and now GGML\n\n-   We can run models on CPUs\n\n    -   No more GPUs needed, they are just a performance boosters\n\n-   Frameworks like Llama.cpp (Python binding), Ollama, GPT4All made it possible\n\n-   But what it takes to run them locally...\n\n    -   will share the experience on Ollama, GPT4All and Llama.cpp\n\n------------------------------------------------------------------------\n\n### Llama.cpp\n\n-   C/C++ interface for running Llama model\n-   binding is available for Python echo system\n-   Runs on most of the platforms (Windows, Linux, MacOS and Docker)\n-   Supports CPU & GPU but need to compile sources for GPU\n-   Plenty of configuration options beyond obvious temp, n_ctx, top_p, top_k etc\n-   Supports 25+ models, 4 multi models, has all major programming language bindings.\n-   [**Observations**]{style=\"color: maroon;\"}\n    -   API based, Easy to use\n    -   No mechanism of auto model download\\\n    -   A bit of slow compared to others frameworks\n\n``` python\nfrom llama_cpp import Llama\n\nllm = Llama(model_path=\"models/llama-2-7b.Q4_K_M.gguf\")\noutput = llm(\"Q: How can you greet someone in Sanskrit? A:\")\nprint(output)\n\n# Output: {'id': 'cmpl-e9db8788-d78f-4832-beb4-74d4f257be3b', 'object': 'text_completion', 'created': 1705823100, 'model': 'models/llama-2-7b.Q4_K_M.gguf', 'choices': [{'text': ' Namaste. surely one of the most well-known greetings from India', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 16, 'total_tokens': 32}}\n```\n\n------------------------------------------------------------------------\n\n### Ollama\n\n-   a tool to run LLMs locally\n-   Runs as a Service and exposes API to interact with models\n-   Provides CLI for managing models\n-   Runs only on Linux and MacOS. Windows support is coming soon\n-   Automatically pulls model through CLI\n-   We can fine-tune and create new custom models just like docker images\n-   Need to interact with Models using REST Api served on `localhost:11434`\n-   Supports majority of models and on both CPU and GPUs\n-   Python binding available `ollama-python` along with LangChain\n\n``` bash\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\":\"How to greet someone in Sanskrit?\",\n  \"stream\":false\n}'\n#{\"model\":\"llama2\",\"created_at\":\"2024-01-21T09:19:18.746918026Z\",\"response\":\"\\nIn Sanskrit, there are several ways to greet someone, depending on the time of day and the level of formality you want to convey. Here are some common Sanskrit greetings:\\n\\n1. \\\"Namaskāra\\\" (नमस्कार): This is a common greeting in Sanskrit, which can be translated to \\\"I bow to you.\\\" It is often used as a general greeting, especially during the daytime.\\n2. \\\"Prāpta\\\" (प्राप्ति): This greeting is used when you want to show respect or gratitude towards someone. It can be translated to \\\"Blessings upon you.\\\"\\n3. \\\"Dhanyavaad\\\" (धन्यवाद): This word means \\\"Thank you\\\" in Sanskrit, and it is often used as a greeting, especially when you want to express your gratitude towards someone.\\n4. \\\"Jaya\\\" (जय): This is a more informal greeting in Sanskrit, which can be translated to \\\"Victory\\\" or \\\"Well done.\\\" It is often used among friends or peers.\\n5. \\\"Shraddhā\\\" (श्रद्धा): This word means \\\"Devotion\\\" or \\\"Respect\\\" in Sanskrit, and it is often used as a greeting when you want to show your devotion or respect towards someone.\\n6. \\\"Maitrī\\\" (मैत्री): This is a friendly greeting in Sanskrit, which can be translated to \\\"Friendship\\\" or \\\"Blessings.\\\" It is often used among friends or peers.\\n7. \\\"Vidhāt\\\" (विधात): This word means \\\"Wishing you\\\" in Sanskrit, and it is often used as a greeting when you want to express your good wishes towards someone.\\n8. \\\"Sukhino\\\" (सुखिनो): This word means \\\"May you be happy\\\" in Sanskrit, and it is often used as a greeting when you want to wish someone happiness or well-being.\\n9. \\\"Bhāvya\\\" (भाव्य): This word means \\\"Wishing you well\\\" in Sanskrit, and it is often used as a greeting when you want to express your good wishes towards someone.\\n10. \\\"Satya\\\" (सत्य): This word means \\\"Truth\\\" or \\\"Reality\\\" in Sanskrit, and it is often used as a greeting when you want to convey the idea of truthfulness or sincerity towards someone.\\n\\nRemember that Sanskrit is a rich and complex language, and there are many other words and phrases that can be used as greetings depending on the context and level of formality you want to convey.\",\"done\":true,\"context\":[518,25580,29962,3532,14816,29903,29958,5299,829,14816,29903,6778,13,13,5328,304,1395,300,4856,297,317,12190,768,29973,518,29914,25580,29962,13,13,797,317,12190,768,29892,727,526,3196,5837,304,1395,300,4856,29892,8679,373,278,931,310,2462,322,278,3233,310,883,2877,366,864,304,27769,29889,2266,526,777,3619,317,12190,768,1395,300,886,29901,13,13,29896,29889,376,29940,314,1278,30107,336,29908,313,30424,30485,30489,30296,30444,30269,30316,1125,910,338,263,3619,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,29902,12580,304,366,1213,739,338,4049,1304,408,263,2498,1395,15133,29892,7148,2645,278,2462,2230,29889,13,29906,29889,376,4040,30107,28363,29908,313,30621,30296,30316,30269,30621,30296,30475,30436,1125,910,1395,15133,338,1304,746,366,864,304,1510,3390,470,20715,4279,7113,4856,29889,739,508,367,20512,304,376,29933,2222,886,2501,366,1213,13,29941,29889,376,29928,29882,1384,879,328,29908,313,31437,30424,30296,30640,30610,30269,30694,1125,910,1734,2794,376,25271,366,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,29892,7148,746,366,864,304,4653,596,20715,4279,7113,4856,29889,13,29946,29889,376,29967,9010,29908,313,30871,30640,1125,910,338,263,901,1871,284,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,29963,919,706,29908,470,376,11284,2309,1213,739,338,4049,1304,4249,7875,470,1236,414,29889,13,29945,29889,376,29903,1092,1202,29882,30107,29908,313,31009,30296,30316,30694,30296,31437,30269,1125,910,1734,2794,376,16618,8194,29908,470,376,1666,1103,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,1510,596,2906,8194,470,3390,7113,4856,29889,13,29953,29889,376,29924,1249,29878,30150,29908,313,30485,31678,30475,30296,30316,30580,1125,910,338,263,19780,1395,15133,297,317,12190,768,29892,607,508,367,20512,304,376,27034,355,3527,29908,470,376,29933,2222,886,1213,739,338,4049,1304,4249,7875,470,1236,414,29889,13,29955,29889,376,29963,333,29882,30107,29873,29908,313,30610,30436,31437,30269,30475,1125,910,1734,2794,376,29956,14424,366,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,4653,596,1781,28688,7113,4856,29889,13,29947,29889,376,29903,2679,29882,1789,29908,313,30489,30702,31667,30436,30424,30799,1125,910,1734,2794,376,12703,366,367,9796,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,6398,4856,22722,470,1532,29899,915,292,29889,13,29929,29889,376,29933,29882,30107,29894,3761,29908,313,31380,30269,30610,30296,30640,1125,910,1734,2794,376,29956,14424,366,1532,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,4653,596,1781,28688,7113,4856,29889,13,29896,29900,29889,376,29903,271,3761,29908,313,30489,30475,30296,30640,1125,910,1734,2794,376,2308,2806,29908,470,376,1123,2877,29908,297,317,12190,768,29892,322,372,338,4049,1304,408,263,1395,15133,746,366,864,304,27769,278,2969,310,8760,1319,2264,470,4457,2265,537,7113,4856,29889,13,13,7301,1096,393,317,12190,768,338,263,8261,322,4280,4086,29892,322,727,526,1784,916,3838,322,12216,2129,393,508,367,1304,408,1395,300,886,8679,373,278,3030,322,3233,310,883,2877,366,864,304,27769,29889],\"total_duration\":163332861673,\"load_duration\":1284681,\"prompt_eval_count\":30,\"prompt_eval_duration\":5579615000,\"eval_count\":621,\"eval_duration\":157747330000}\n```\n\n------------------------------------------------------------------------\n\n### GPT4All\n\n-   from Nomic AI\n-   Provides an echo system from training of a LLM to framework to interact\n-   supports major models including their own trained models\n-   Supports Windows, Linux and Mac Platforms\n-   Provides Python framework for interacting with model\n-   Automatically download models from internet\n-   Supports both CPU and GPUs\n-   [**Observations**]{style=\"color: maroon;\"}\n    -   Better performance than Llama.cpp and Ollama\n    -   Needs more GPU memory if we want to load model into GPU\n    -   Easy to use API\n\n``` python\nfrom gpt4all import GPT4All\n\nllm = GPT4All(model_name=\"mistral-7b-openorca.Q4_0.gguf\", allow_download=False)\noutput = llm.generate(\"Q: How can you greet someone in Sanskrit? A:\")\nprint(output)\n\n#To greet someone in Sanskrit, you can say \"Namaste\" (नमस्ते).\n```\n\n------------------------------------------------------------------------\n\n### P&C of local LLMs\n\n-   **Pros**\n    -   No Subscriptions & Rate Limitations\n    -   Privacy & Security in control. Data will not move anywhere\n    -   Quick Prototype and Evaluation\n    -   More control on turning\n-   **Cons**\n    -   Needs to know internals for turning\n    -   On our own for Deployment & Scalability aspects\n\n------------------------------------------------------------------------\n\n### Environment used for Evaluation\n\n-   **Hardware**\n    -   GPU - Nvidia GeoForce RTX 4 GB\n    -   Intel i5 Processor, 6 core processor\n    -   32 GB RAM\n-   **Software**\n    -   Ubuntu OS\n    -   CUDA Drivers\n    -   Python 3.10\n\n------------------------------------------------------------------------\n\n### Models\n\n-   Llama 2 - Chat & Instructions\n    -   7B & 13B Parameters\n-   CodeLlama 13B - Code generation\n-   Mistral 7B - Chat & Instructions\n-   all-MiniLM-L6-v2-f16 - for Embedding\n-   BakLLaVA - Multi Model for image inferences\n\n------------------------------------------------------------------------\n\n### PoC-1 KnowledgeBase QA Bot (RAG)\n\n::: columns\n::: {.column width=\"50%\"}\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Creating a QA Bot on local documents using Retrieval Augmented Generation using Local LLM.                             |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, Ollama, Streamlit                                                                                           |\n| **Models**       | Llama2 for chat, all-MiniLM-L6-v2-f16 for embedding                                                                    |\n| **Vectorstore**  | FAISS, Chroma                                                                                                          |\n| **Observations** | \\- Chroma stores data in SQLite where FAISS persists data in pickle files                                              |\n|                  | \\- Chroma has decent persist method to persist data in between the ingestion while FAISS need to only once(at the end) |\n|                  | \\- FAISS results are better when copared to Chroma                                                                     |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n:::\n\n::: {.column width=\"50%\"}\n![](./images/rag.png)\n:::\n:::\n\n------------------------------------------------------------------------\n\n### PoC-2 Code documentation Generation\n\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Generating JavaDoc for a Legacy undocumented Java Code                                                                                 |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, Gpt4All                                                                                                                     |\n| **Models**       | CodeLlama for code interpretation, Forge Roaster for Java Source Code parsing                                                          |\n| **Observations** | \\- LLMs tries to modify the source code even though mentioned not to in the prompt                                                     |\n|                  | \\- Balancing the JavaDoc documentation is tricky as part of prompt                                                                     |\n|                  | \\- A Context about the project purpose, components, architecture able to help to add little more meaning to docs.                      |\n|                  | \\- Few Shot Prompting technique helps to give specific instructs like not to generate JavaDoc for Entity accessor and mutator methods. |\n|                  | \\- Can't just trust LLM to modify the original code.                                                                                    |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n\n![](./images/codegen.png){width=\"100%\"}\n\n---\n\n### PoC-3 Data Extraction from PDFs\n\n```{=html}\n<style>\ntd, th {\n   border: none!important;\n   font-size: 22px\n}\n\n</style>\n```\n| **Objective**    | Extraction of Data from Documents                                                |\n|-----------------------------------------|-------------------------------|\n| **Tools**        | LangChain, OpenAI, Unstructured                                                  |\n| **Models**       | gpt-3.5                                                                          |\n| **Observations** | \\- Extract data from Amazon Invoice PDF files                                    |\n|                  | \\- OpenAI Functions works well in this scenario                                  |\n|                  | \\- Subsequent process automation can be enabled using Langchain tools and agents |\n|                  | \\- Defining JSON schema will become tricky if Document data is complex           |\n|                  |                                                                                  |\n\n: {tbl-colwidths=\"\\[25,75\\]\" .borderless}\n\n::: columns\n::: {.column width=\"50%\"}\n![](./images/invoice.png){width=\"50%\"}\n:::\n\n::: {.column width=\"50%\"}\n``` json\n{\n   \"Billing Address\":\"Telkapalli Venkata Seshagiri H.No.3-10-21/A, Gokhale Nagar,, Ramanthapur Hyderabad, ANDHRA PRADESH, 500013 IN\",\n   \"PAN No\":\"AAPCA6346P\",\n   \"GST Registration No\":\"36AAPCA6346P1ZW\",\n   \"Order Number\":\"407-9397603-5919514\",\n   \"Invoice Number\":\"IN-HYD8-41962\",\n   \"Invoice Date\":\"08.04.2020\",\n   \"orderItems\":[\n      {\n         \"Sl. No\":\"1\",\n         \"Description\":\"Dark Fantasy Choco Fills, 300g | B01L7A0CU4 ( B01L7A0CU4 )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹83.90\",\n         \"Qty\":\"2\",\n         \"Net Amount\":\"₹167.80\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹15.10\",\n         \"Total Amount\":\"₹198.00\"\n      },\n      {\n         \"Sl. No\":\"2\",\n         \"Description\":\"Madhur Pure Sugar, 5kg Bag | B01GCF0XEY ( B01GCF0XEY )\",\n         \"HSN\":\"1701\",\n         \"Unit Price\":\"₹228.58\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹228.58\",\n         \"Tax Rate\":\"2.5%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹5.71\",\n         \"Total Amount\":\"₹240.00\"\n      },\n      {\n         \"Sl. No\":\"3\",\n         \"Description\":\"Sunfeast Dark Fantasy Choco Fills plus Coffee Fills Combo 75g(Buy 3 Get 1 Free) | B07KGG57RL ( B07KGG57RL )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹76.28\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹76.28\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹6.86\",\n         \"Total Amount\":\"₹90.00\"\n      },\n      {\n         \"Sl. No\":\"4\",\n         \"Description\":\"McVities Fruit Cookie, 120g | B01NCB1OCV ( B01NCB1OCV )\",\n         \"HSN\":\"1905\",\n         \"Unit Price\":\"₹33.90\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹33.90\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹3.05\",\n         \"Total Amount\":\"₹40.00\"\n      },\n      {\n         \"Sl. No\":\"5\",\n         \"Description\":\"Kellogg's Corn Flakes Real Almond and Honey, 1kg |B01H5LBWG2 ( B01H5LBWG2 )\",\n         \"HSN\":\"1904\",\n         \"Unit Price\":\"₹396.62\",\n         \"Qty\":\"1\",\n         \"Net Amount\":\"₹396.62\",\n         \"Tax Rate\":\"9%\",\n         \"Tax Type\":\"CGST\",\n         \"Tax Amount\":\"₹35.69\",\n         \"Total Amount\":\"₹468.00\"\n      }\n   ],\n   \"Amount in Words\":\"One Thousand And Thirty-six only\"\n}\n```\n:::\n:::\n\n------------------------------------------------------------------------\n\n### Limitations\n\n-   Need more Hardware for faster response and big models\n-   Though Function calling supported in Llama based models, its not as powerful as OpenAI and others\n-   Prompt Format varies between Model, need to get hold of it\n-   Need to get hold of model parameters for fine-tuning\n\n------------------------------------------------------------------------\n\n### LangChain\n\n-   Defacto for LLM orchestration\n-   Very minimal code changes for switching between LLM models\n-   Frequent updates/releases, started with 0.0.34, got new one 0.0.35 and sooner got 0.1.0\n-   Some Code examples refers old modules and classes\n-   Some Examples has issues and will not go quite well if we do on different data\n-   Most of examples are with OpenAI\n-   Most of the documentation is notebooks\n-   One need to have good Python understanding to get hold of source code and code documentation\n-   Debugging with LangSmith is not easy at beginning and waiting list for getting Access Key\n\n------------------------------------------------------------------------\n\n### Skills needed\n\n-   AI/ML background is an optional (very minimal)\n\n-   Good Python programming background is a must\n\n-   If want more adventurous life, sure! LangChain NodeJS framework is a best bet\n\n-   Prompt Engineering theory is entirely different from practical scenarios\n\n-   Hence Intro. to Prompt Engineering or GenAI courses may help to understand terms but not in programming\n\n- Formulating Prompts and Prompt Engineering techniques needs patience. No sure shot syntax\n\n--- \n\n### Conclusion\n\n-   GenAI programming is fun but not quite easy.\n-   Talking about it is different from doing it.\n-   Showing a real value with it takes a while. (well, ChatBot is not really the ones)\n-   But we need to feel it by experience, aka by doing it.\n\n🙏🙏🙏"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["../../../custom/css/revealjs.css"],"output-file":"localllms.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.551","auto-stretch":true,"output-dir":"../..","slideNumber":false,"theme":"dark","smaller":true,"title":"Local LLMs"}}},"projectFormats":["html"]}